{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Fine-Tuning with SFTTrainer\n",
    "\n",
    "This notebook demonstrates how to fine-tune the `HuggingFaceTB/SmolLM2-135M` model using the `SFTTrainer` from the `trl` library. The notebook cells run and will finetune the model. You can select your difficulty by trying out different datasets.\n",
    "\n",
    "<div style='background-color: lightblue; padding: 10px; border-radius: 5px; margin-bottom: 20px; color:black'>\n",
    "    <h2 style='margin: 0;color:blue'>Exercise: Fine-Tuning SmolLM2 with SFTTrainer</h2>\n",
    "    <p>Take a dataset from the Hugging Face hub and finetune a model on it. </p> \n",
    "    <p><b>Difficulty Levels</b></p>\n",
    "    <p>üê¢ Use the `HuggingFaceTB/smoltalk` dataset</p>\n",
    "    <p>üêï Try out the `bigcode/the-stack-smol` dataset and finetune a code generation model on a specific subset `data/python`.</p>\n",
    "    <p>ü¶Å Select a dataset that relates to a real world use case your interested in</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c903f3b4c244237be529c5529ed49df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Install the requirements in Google Colab\n",
    "# !pip install transformers datasets trl huggingface_hub\n",
    "\n",
    "# Authenticate to Hugging Face\n",
    "\n",
    "from huggingface_hub import login\n",
    "login()\n",
    "\n",
    "# for convenience you can create an environment variable containing your hub token as HF_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from trl import SFTConfig, SFTTrainer, setup_chat_format\n",
    "import torch\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_name = \"HuggingFaceTB/SmolLM2-135M\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_name\n",
    ").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_name)\n",
    "\n",
    "# Set up the chat format\n",
    "model, tokenizer = setup_chat_format(model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Set our name for the finetune to be saved &/ uploaded to\n",
    "finetune_name = \"SmolLM2-FT-MyDataset\"\n",
    "finetune_tags = [\"smol-course\", \"module_1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate with the base model\n",
    "\n",
    "Here we will try out the base model which does not have a chat template. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training:\n",
      "user\n",
      "Write a haiku about programming\n",
      "assistant\n",
      "Write a haiku about programming\n",
      "assistant\n",
      "Write a haiku about programming\n",
      "assistant\n",
      "Write a haiku about programming\n",
      "assistant\n",
      "Write a haiku about programming\n",
      "assistant\n",
      "Write a haiku about programming\n",
      "assistant\n",
      "Write a haiku about programming\n",
      "assistant\n",
      "Write a haiku about programming\n",
      "assistant\n",
      "Write a haiku about programming\n",
      "assistant\n",
      "Write a haiku about programming\n",
      "assistant\n",
      "Write a haiku about programming\n",
      "assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's test the base model before training\n",
    "prompt = \"Write a haiku about programming\"\n",
    "\n",
    "# Format with template\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "formatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "# Generate response\n",
    "inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "print(\"Before training:\")\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mps'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# A haiku about 1000 words.\\n\\nA haiku is a short poem that is usually about 100 words. It is a Japanese word that means ‚Äúlittle poem.‚Äù\\n\\nA haiku is a very simple poem. It is a short poem that is about 100 words.\\n\\nA haiku is a very simple poem. It is a short poem that is about 100 words.\\n\\nA haiku is a very simple poem. It is a'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(\"# A haiku about \", return_tensors=\"pt\").to(device)\n",
    "inputs\n",
    "outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "outputs\n",
    "tokenizer.decode(outputs[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   19,   330,   421, 30614,   563,  6256,    30,  1116,   198,    19,\n",
       "           330,   421, 30614,   563,  6256,    30,   198,   198,    19,   330,\n",
       "           421, 30614,   563,  6256,    30,   198,   198,    19,   330,   421,\n",
       "         30614,   563,  6256,    30,   198,   198,    19,   330,   421, 30614,\n",
       "           563,  6256,    30,   198,   198,    19,   330,   421, 30614,   563,\n",
       "          6256,    30,   198,   198,    19,   330,   421, 30614,   563,  6256,\n",
       "            30,   198,   198,    19,   330,   421, 30614,   563,  6256,    30,\n",
       "           198,   198,    19,   330,   421, 30614,   563,  6256,    30,   198,\n",
       "           198,    19,   330,   421, 30614,   563,  6256,    30,   198,   198,\n",
       "            19,   330,   421, 30614,   563,  6256,    30,   198,   198,    19,\n",
       "           330,   421, 30614,   563,  6256,    30,   198,   198]],\n",
       "       device='mps:0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# A haiku about programming.\\n\\n\\n# A haiku about programming.\\n\\n# A haiku about programming.\\n\\n# A haiku about programming.\\n\\n# A haiku about programming.\\n\\n# A haiku about programming.\\n\\n# A haiku about programming.\\n\\n# A haiku about programming.\\n\\n# A haiku about programming.\\n\\n# A haiku about programming.\\n\\n# A haiku about programming.\\n\\n# A haiku about programming.\\n\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation\n",
    "\n",
    "We will load a sample dataset and format it for training. The dataset should be structured with input-output pairs, where each input is a prompt and the output is the expected response from the model.\n",
    "\n",
    "**TRL will format input messages based on the model's chat templates.** They need to be represented as a list of dictionaries with the keys: `role` and `content`,."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sample dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "# TODO: define your dataset and config using the path and name parameters\n",
    "ds = load_dataset(path=\"HuggingFaceTB/smoltalk\", name=\"everyday-conversations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: ü¶Å Ifrom datasets import load_dataset\n",
    "\n",
    "def process_dataset(sample):\n",
    "    print(type(sample))\n",
    "    # print(sample)\n",
    "    print(list(sample.keys()))\n",
    "\n",
    "    sample['messages'] = tokenizer.apply_chat_template(sample['messages'], tokenize=False)\n",
    "    return sample\n",
    "                                            # TODO: üê¢ Convert the sample into a chat format\n",
    "    # use the tokenizer's method to apply the chat template\n",
    "    # return sample\n",
    "\n",
    "\n",
    "ds = ds.map(process_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring the SFTTrainer\n",
    "\n",
    "The `SFTTrainer` is configured with various parameters that control the training process. These include the number of training steps, batch size, learning rate, and evaluation strategy. Adjust these parameters based on your specific requirements and computational resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/manuel/.pyenv/versions/3.11.4/lib/python3.11/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/Users/manuel/.pyenv/versions/3.11.4/lib/python3.11/site-packages/transformers/training_args.py:2248: UserWarning: `use_mps_device` is deprecated and will be removed in version 5.0 of ü§ó Transformers. `mps` device will be used by default if available similar to the way `cuda` device is used.Therefore, no action from user is required. \n",
      "  warnings.warn(\n",
      "/Users/manuel/.pyenv/versions/3.11.4/lib/python3.11/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field. Will not be supported from version '0.13.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/Users/manuel/.pyenv/versions/3.11.4/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:309: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n",
      "/Users/manuel/.pyenv/versions/3.11.4/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:328: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "024dbedc13244fe48b5dcca4c6815810",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/119 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "# Configure the SFTTrainer\n",
    "sft_config = SFTConfig(\n",
    "    output_dir=\"./sft_output\",\n",
    "    max_steps=1000,  # Adjust based on dataset size and desired training duration\n",
    "    per_device_train_batch_size=8,  # Set according to your GPU memory capacity\n",
    "    learning_rate=5e-5,  # Common starting point for fine-tuning\n",
    "    logging_steps=10,  # Frequency of logging training metrics\n",
    "    save_steps=100,  # Frequency of saving model checkpoints\n",
    "    evaluation_strategy=\"steps\",  # Evaluate the model at regular intervals\n",
    "    eval_steps=50,  # Frequency of evaluation\n",
    "    use_mps_device=(\n",
    "        True if device == \"mps\" else False\n",
    "    ),  # Use MPS for mixed precision training\n",
    "    hub_model_id=finetune_name,  # Set a unique name for your model\n",
    ")\n",
    "\n",
    "# Initialize the SFTTrainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=sft_config,\n",
    "    train_dataset=ds[\"train\"],\n",
    "    tokenizer=tokenizer,\n",
    "    eval_dataset=ds[\"test\"],\n",
    "    dataset_text_field='messages'\n",
    ")\n",
    "\n",
    "# TODO: ü¶Å üêï align the SFTTrainer params with your chosen dataset. For example, if you are using the `bigcode/the-stack-smol` dataset, you will need to choose the `content` column`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "With the trainer configured, we can now proceed to train the model. The training process will involve iterating over the dataset, computing the loss, and updating the model's parameters to minimize this loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 21:43, Epoch 3/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.116800</td>\n",
       "      <td>1.135888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.064400</td>\n",
       "      <td>1.086552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.039400</td>\n",
       "      <td>1.067214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.003000</td>\n",
       "      <td>1.057764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.069300</td>\n",
       "      <td>1.039805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.815300</td>\n",
       "      <td>1.038922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.812200</td>\n",
       "      <td>1.035872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.856800</td>\n",
       "      <td>1.029380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.847500</td>\n",
       "      <td>1.027021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.839700</td>\n",
       "      <td>1.023372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.849100</td>\n",
       "      <td>1.017103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.713300</td>\n",
       "      <td>1.032182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.721900</td>\n",
       "      <td>1.035051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.684600</td>\n",
       "      <td>1.032822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.703500</td>\n",
       "      <td>1.030352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.733200</td>\n",
       "      <td>1.031005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.685800</td>\n",
       "      <td>1.029206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.663200</td>\n",
       "      <td>1.040233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.667400</td>\n",
       "      <td>1.041985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.700800</td>\n",
       "      <td>1.041812</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "trainer.save_model(f\"./{finetune_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e858520af4e9449a83647f3d6fddf2dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/538M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c988f78671140c4bb86b91c67b29f01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "800bea7ceb29493db0788f9cd0bcef84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/5.56k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/wesen3000/SmolLM2-FT-MyDataset/commit/96f53e6c7e980e2b51e6a4c5b37dda0978da2d30', commit_message='End of training', commit_description='', oid='96f53e6c7e980e2b51e6a4c5b37dda0978da2d30', pr_url=None, repo_url=RepoUrl('https://huggingface.co/wesen3000/SmolLM2-FT-MyDataset', endpoint='https://huggingface.co', repo_type='model', repo_id='wesen3000/SmolLM2-FT-MyDataset'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub(tags=finetune_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='background-color: lightblue; padding: 10px; border-radius: 5px; margin-bottom: 20px; color:black'>\n",
    "    <h2 style='margin: 0;color:blue'>Bonus Exercise: Generate with fine-tuned model</h2>\n",
    "    <p>üêï Use the fine-tuned to model generate a response, just like with the base example..</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normal: user\n",
      "Write a haiku about programming\n",
      "assistant\n",
      "Write a haiku about programming\n",
      "assistant\n",
      "Write a haiku about programming\n",
      "assistant\n",
      "Write a haiku about programming\n",
      "assistant\n",
      "Write a haiku about programming\n",
      "assistant\n",
      "Write a haiku about programming\n",
      "assistant\n",
      "Write a haiku about programming\n",
      "assistant\n",
      "Write a haiku about programming\n",
      "assistant\n",
      "Write a haiku about programming\n",
      "assistant\n",
      "Write a haiku about programming\n",
      "assistant\n",
      "Write a haiku about programming\n",
      "assistant\n",
      "\n",
      "finetuned: user\n",
      "Write a haiku about programming\n",
      "assistant\n",
      "I'm a language model, and I'm looking for some programming haikus. Do you have any suggestions?\n",
      "\n",
      "Yes, I can suggest some. One popular one is \"Hello World\" by John Gruber. It's a classic and easy to learn.\n",
      "user\n",
      "That sounds great. What's the main idea of that haiku?\n",
      "assistant\n",
      "The main idea of that haiku is to say that programming is like writing a simple program. It's a\n"
     ]
    }
   ],
   "source": [
    "# Test the fine-tuned model on the same prompt\n",
    "\n",
    "# Let's test the base model before training\n",
    "prompt = \"Write a haiku about programming\"\n",
    "\n",
    "# Format with template\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "formatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "# Load the model and tokenizer\n",
    "finetuned_model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=f\"./{finetune_name}\"\n",
    ").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=f\"./{finetune_name}\")\n",
    "\n",
    "# Generate response\n",
    "inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "outputs\n",
    "print(\"normal: \" + tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "\n",
    "outputs = finetuned_model.generate(**inputs, max_new_tokens=100)\n",
    "outputs\n",
    "print(\"finetuned: \" + tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "\n",
    "\n",
    "# TODO: use the fine-tuned to model generate a response, just like with the base example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíê You're done!\n",
    "\n",
    "This notebook provided a step-by-step guide to fine-tuning the `HuggingFaceTB/SmolLM2-135M` model using the `SFTTrainer`. By following these steps, you can adapt the model to perform specific tasks more effectively. If you want to carry on working on this course, here are steps you could try out:\n",
    "\n",
    "- Try this notebook on a harder difficulty\n",
    "- Review a colleagues PR\n",
    "- Improve the course material via an Issue or PR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8879de7e68c4c38b6177ef507c456c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/3.30k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e94e93f00244eaba3d300c2fdb501da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data.json:   0%|          | 0.00/112M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5180bd15a57e4e199a84cfe7bbe1cffb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load a sample dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "# TODO: define your dataset and config using the path and name parameters\n",
    "ds2 = load_dataset(path=\"bigcode/the-stack-smol\", data_dir=\"data/go\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': 'package auth\\n\\nimport (\\n\\t\"fmt\"\\n\\t\"strings\"\\n\\n\\t\"github.com/pkg/errors\"\\n\\t\"github.com/rancher/types/apis/management.cattle.io/v3\"\\n\\t\"github.com/sirupsen/logrus\"\\n\\t\"k8s.io/apimachinery/pkg/apis/meta/v1\"\\n\\t\"k8s.io/apimachinery/pkg/labels\"\\n)\\n\\nconst (\\n\\tclusterResource           = \"clusters\"\\n\\tmembershipBindingOwner    = \"memberhsip-binding-owner\"\\n\\tcrtbInProjectBindingOwner = \"crtb-in-project-binding-owner\"\\n\\tprtbInClusterBindingOwner = \"prtb-in-cluster-binding-owner\"\\n\\trbByOwnerIndex            = \"auth.management.cattle.io/rb-by-owner\"\\n\\trbByRoleAndSubjectIndex   = \"auth.management.cattle.io/crb-by-role-and-subject\"\\n\\tctrbMGMTController        = \"mgmt-auth-crtb-controller\"\\n)\\n\\nvar clusterManagmentPlaneResources = []string{\"clusterroletemplatebindings\", \"nodes\", \"nodepools\", \"clusterevents\",\\n\\t\"projects\", \"clusterregistrationtokens\", \"clusterloggings\", \"notifiers\", \"clusteralerts\",\\n\\t\"podsecuritypolicytemplateprojectbindings\"}\\n\\ntype crtbLifecycle struct {\\n\\tmgr           *manager\\n\\tclusterLister v3.ClusterLister\\n}\\n\\nfunc (c *crtbLifecycle) Create(obj *v3.ClusterRoleTemplateBinding) (*v3.ClusterRoleTemplateBinding, error) {\\n\\tobj, err := c.reconcileSubject(obj)\\n\\tif err != nil {\\n\\t\\treturn nil, err\\n\\t}\\n\\terr = c.reconcilBindings(obj)\\n\\n\\treturn obj, err\\n}\\n\\nfunc (c *crtbLifecycle) Updated(obj *v3.ClusterRoleTemplateBinding) (*v3.ClusterRoleTemplateBinding, error) {\\n\\tobj, err := c.reconcileSubject(obj)\\n\\tif err != nil {\\n\\t\\treturn nil, err\\n\\t}\\n\\terr = c.reconcilBindings(obj)\\n\\treturn obj, err\\n}\\n\\nfunc (c *crtbLifecycle) Remove(obj *v3.ClusterRoleTemplateBinding) (*v3.ClusterRoleTemplateBinding, error) {\\n\\tif err := c.mgr.reconcileClusterMembershipBindingForDelete(\"\", string(obj.UID)); err != nil {\\n\\t\\treturn nil, err\\n\\t}\\n\\terr := c.removeMGMTClusterScopedPrivilegesInProjectNamespace(obj)\\n\\treturn nil, err\\n}\\n\\nfunc (c *crtbLifecycle) reconcileSubject(binding *v3.ClusterRoleTemplateBinding) (*v3.ClusterRoleTemplateBinding, error) {\\n\\tif binding.GroupName != \"\" || binding.GroupPrincipalName != \"\" || (binding.UserPrincipalName != \"\" && binding.UserName != \"\") {\\n\\t\\treturn binding, nil\\n\\t}\\n\\n\\tif binding.UserPrincipalName != \"\" && binding.UserName == \"\" {\\n\\t\\tdisplayName := binding.Annotations[\"auth.cattle.io/principal-display-name\"]\\n\\t\\tuser, err := c.mgr.userMGR.EnsureUser(binding.UserPrincipalName, displayName)\\n\\t\\tif err != nil {\\n\\t\\t\\treturn binding, err\\n\\t\\t}\\n\\n\\t\\tbinding.UserName = user.Name\\n\\t\\treturn binding, nil\\n\\t}\\n\\n\\tif binding.UserPrincipalName == \"\" && binding.UserName != \"\" {\\n\\t\\tu, err := c.mgr.userLister.Get(\"\", binding.UserName)\\n\\t\\tif err != nil {\\n\\t\\t\\treturn binding, err\\n\\t\\t}\\n\\t\\tfor _, p := range u.PrincipalIDs {\\n\\t\\t\\tif strings.HasSuffix(p, binding.UserName) {\\n\\t\\t\\t\\tbinding.UserPrincipalName = p\\n\\t\\t\\t\\tbreak\\n\\t\\t\\t}\\n\\t\\t}\\n\\t\\treturn binding, nil\\n\\t}\\n\\n\\treturn nil, errors.Errorf(\"Binding %v has no subject\", binding.Name)\\n}\\n\\n// When a CRTB is created or updated, translate it into several k8s roles and bindings to actually enforce the RBAC\\n// Specifically:\\n// - ensure the subject can see the cluster in the mgmt API\\n// - if the subject was granted owner permissions for the clsuter, ensure they can create/update/delete the cluster\\n// - if the subject was granted privileges to mgmt plane resources that are scoped to the cluster, enforce those rules in the cluster\\'s mgmt plane namespace\\nfunc (c *crtbLifecycle) reconcilBindings(binding *v3.ClusterRoleTemplateBinding) error {\\n\\tif binding.UserName == \"\" && binding.GroupPrincipalName == \"\" && binding.GroupName == \"\" {\\n\\t\\treturn nil\\n\\t}\\n\\n\\tclusterName := binding.ClusterName\\n\\tcluster, err := c.clusterLister.Get(\"\", clusterName)\\n\\tif err != nil {\\n\\t\\treturn err\\n\\t}\\n\\tif cluster == nil {\\n\\t\\treturn errors.Errorf(\"cannot create binding because cluster %v was not found\", clusterName)\\n\\t}\\n\\n\\tisOwnerRole := binding.RoleTemplateName == \"cluster-owner\"\\n\\tvar clusterRoleName string\\n\\tif isOwnerRole {\\n\\t\\tclusterRoleName = strings.ToLower(fmt.Sprintf(\"%v-clusterowner\", clusterName))\\n\\t} else {\\n\\t\\tclusterRoleName = strings.ToLower(fmt.Sprintf(\"%v-clustermember\", clusterName))\\n\\t}\\n\\n\\tsubject, err := buildSubjectFromRTB(binding)\\n\\tif err != nil {\\n\\t\\treturn err\\n\\t}\\n\\tif err := c.mgr.ensureClusterMembershipBinding(clusterRoleName, string(binding.UID), cluster, isOwnerRole, subject); err != nil {\\n\\t\\treturn err\\n\\t}\\n\\n\\terr = c.mgr.grantManagementPlanePrivileges(binding.RoleTemplateName, clusterManagmentPlaneResources, subject, binding)\\n\\tif err != nil {\\n\\t\\treturn err\\n\\t}\\n\\n\\tprojects, err := c.mgr.projectLister.List(binding.Namespace, labels.Everything())\\n\\tif err != nil {\\n\\t\\treturn err\\n\\t}\\n\\tfor _, p := range projects {\\n\\t\\tif err := c.mgr.grantManagementClusterScopedPrivilegesInProjectNamespace(binding.RoleTemplateName, p.Name, projectManagmentPlaneResources, subject, binding); err != nil {\\n\\t\\t\\treturn err\\n\\t\\t}\\n\\t}\\n\\treturn nil\\n}\\n\\nfunc (c *crtbLifecycle) removeMGMTClusterScopedPrivilegesInProjectNamespace(binding *v3.ClusterRoleTemplateBinding) error {\\n\\tprojects, err := c.mgr.projectLister.List(binding.Namespace, labels.Everything())\\n\\tif err != nil {\\n\\t\\treturn err\\n\\t}\\n\\tfor _, p := range projects {\\n\\t\\tset := labels.Set(map[string]string{string(binding.UID): crtbInProjectBindingOwner})\\n\\t\\trbs, err := c.mgr.rbLister.List(p.Name, set.AsSelector())\\n\\t\\tif err != nil {\\n\\t\\t\\treturn err\\n\\t\\t}\\n\\t\\tfor _, rb := range rbs {\\n\\t\\t\\tlogrus.Infof(\"[%v] Deleting rolebinding %v in namespace %v for crtb %v\", ctrbMGMTController, rb.Name, p.Name, binding.Name)\\n\\t\\t\\tif err := c.mgr.mgmt.RBAC.RoleBindings(p.Name).Delete(rb.Name, &v1.DeleteOptions{}); err != nil {\\n\\t\\t\\t\\treturn err\\n\\t\\t\\t}\\n\\t\\t}\\n\\t}\\n\\treturn nil\\n}\\n',\n",
       " 'avg_line_length': 33.1393939394,\n",
       " 'max_line_length': 172,\n",
       " 'alphanum_fraction': 0.7267739576,\n",
       " 'licenses': ['Apache-2.0'],\n",
       " 'repository_name': 'cloudnautique/rancher',\n",
       " 'path': 'pkg/controllers/management/auth/crtb_handler.go',\n",
       " 'size': 5468,\n",
       " 'lang': 'GO'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds2['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/manuel/.pyenv/versions/3.11.4/lib/python3.11/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field. Will not be supported from version '0.13.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/Users/manuel/.pyenv/versions/3.11.4/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:309: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n",
      "/Users/manuel/.pyenv/versions/3.11.4/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:328: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21256b87dba746f4b89aa3e6798fedac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53954d552a8a4173885f9abb34b7af25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "ds2_split = ds2[\"train\"].train_test_split(test_size=0.2)\n",
    "\n",
    "# Configure the SFTTrainer\n",
    "sft_config = SFTConfig(\n",
    "    output_dir=\"./sft_output\",\n",
    "    max_steps=1000,  # Adjust based on dataset size and desired training duration\n",
    "    per_device_train_batch_size=8,  # Set according to your GPU memory capacity\n",
    "    learning_rate=5e-5,  # Common starting point for fine-tuning\n",
    "    logging_steps=10,  # Frequency of logging training metrics\n",
    "    save_steps=100,  # Frequency of saving model checkpoints\n",
    "    evaluation_strategy=\"steps\",  # Evaluate the model at regular intervals\n",
    "    eval_steps=50,  # Frequency of evaluation\n",
    "    use_mps_device=(\n",
    "        True if device == \"mps\" else False\n",
    "    ),  # Use MPS for mixed precision training\n",
    "    hub_model_id=finetune_name,  # Set a unique name for your model\n",
    ")\n",
    "\n",
    "# Initialize the SFTTrainer\n",
    "trainer2 = SFTTrainer(\n",
    "    model=model,\n",
    "    args=sft_config,\n",
    "    train_dataset=ds2_split[\"train\"],\n",
    "    tokenizer=tokenizer,\n",
    "    eval_dataset=ds2_split[\"test\"],\n",
    "    dataset_text_field='content'\n",
    ")\n",
    "\n",
    "# TODO: ü¶Å üêï align the SFTTrainer params with your chosen dataset. For example, if you are using the `bigcode/the-stack-smol` dataset, you will need to choose the `content` column`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 18:19, Epoch 3/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.639300</td>\n",
       "      <td>1.069599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.662800</td>\n",
       "      <td>1.059011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.667600</td>\n",
       "      <td>1.060931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.652100</td>\n",
       "      <td>1.064154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.732100</td>\n",
       "      <td>1.051247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.486600</td>\n",
       "      <td>1.091722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.482800</td>\n",
       "      <td>1.109315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.531500</td>\n",
       "      <td>1.114455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.538300</td>\n",
       "      <td>1.109027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.523200</td>\n",
       "      <td>1.109667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.545600</td>\n",
       "      <td>1.106174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.401600</td>\n",
       "      <td>1.168546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.415900</td>\n",
       "      <td>1.180274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.390500</td>\n",
       "      <td>1.176039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.416000</td>\n",
       "      <td>1.170217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.432500</td>\n",
       "      <td>1.175107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.410900</td>\n",
       "      <td>1.175115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.370200</td>\n",
       "      <td>1.213978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.370900</td>\n",
       "      <td>1.220102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.338400</td>\n",
       "      <td>1.220589</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()\n",
    "trainer.save_model(f\"./{finetune_name}-go-stack\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go stack: func testDatabaseConnection(ctx *context.Context) {\n",
      "    if (context.Database != context.Database) {\n",
      "        context.Database = context.Context.getDatabase();\n",
      "    }\n",
      "\n",
      "    return context.Database;\n",
      "}\n",
      "\n",
      "/**\n",
      " * @param ctx\n",
      " *            The context to use for the test\n",
      " */\n",
      "void testDatabaseConnection(Context ctx) {\n",
      "    ctx.Database = testDatabaseConnection(ctx);\n",
      "}\n",
      "\n",
      "/**\n",
      " * @param ctx\n",
      " *            The context to use for the test\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the fine-tuned model on the same prompt\n",
    "\n",
    "# Let's test the base model before training\n",
    "prompt = \"func testDatabaseConnection(ctx *context.Context)\"\n",
    "\n",
    "# Format with template\n",
    "\n",
    "# Load the model and tokenizer\n",
    "finetuned_go_model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=f\"./{finetune_name}-go-stack\"\n",
    ").to(device)\n",
    "\n",
    "# Generate response\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "outputs = finetuned_go_model.generate(**inputs, max_new_tokens=100)\n",
    "outputs\n",
    "print(\"go stack: \" + tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
